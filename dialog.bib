@article{ChenHTCGD16,
author = {Chen, Yun-Nung and Hakkani-T{\"{u}}r, Dilek Z and T{\"{u}}r, G{\"{o}}khan and {\c{C}}elikyilmaz, Asli and Gao, Jianfeng and Deng, Li},
journal = {CoRR},
title = {{Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks}},
url = {http://arxiv.org/abs/1609.03286},
volume = {abs/1609.0},
year = {2016}
}
@inproceedings{chen2016,
abstract = {Spoken language understanding (SLU) is a core component of a spoken dialogue system. In the traditional architecture of dialogue systems, the SLU component treats each utterance independent of each other, and then the following components aggregate the multi-turn information in the separate phases. However, there are two challenges: 1) errors from previous turns may be propagated and then degrade the performance of the current turn; 2) knowledge mentioned in the long history may not be carried into the current turn. This paper addresses the above issues by proposing an architecture using end-to-end memory networks to model knowledge carryover in multi-turn conversations, where utterances encoded with intents and slots can be stored as embeddings in the memory and the decoding phase applies an attention model to leverage previously stored semantics for intent prediction and slot tagging simultaneously. The experiments on Microsoft Cortana conversational data show that the proposed memory network architecture can effectively extract salient semantics for modeling knowledge carryover in the multi-turn conversations and outperform the results using the state-of-the-art recurrent neural network framework (RNN) designed for single-turn SLU.},
author = {Chen, Yun-Nung and Hakkani-T{\"{u}}r, Dilek Z and T{\"{u}}r, Gokhan and Gao, Jianfeng and Deng, Li},
month = {jun},
publisher = {ISCA},
title = {{End-to-End Memory Networks with Knowledge Carryover for Multi-Turn Spoken Language Understanding}},
url = {https://www.microsoft.com/en-us/research/publication/contextualslu/},
year = {2016}
}
@article{YangCHCLGD16,
author = {Yang, Xuesong and Chen, Yun-Nung and Hakkani-T{\"{u}}r, Dilek Z and Crook, Paul and Li, Xiujun and Gao, Jianfeng and Deng, Li},
journal = {CoRR},
title = {{End-to-End Joint Learning of Natural Language Understanding and Dialogue Manager}},
url = {http://arxiv.org/abs/1612.00913},
volume = {abs/1612.00913},
year = {2016}
}
@article{wen2017latent,
author = {Wen, Tsung-Hsien and Miao, Yishu and Blunsom, Phil and Young, Steve},
journal = {arXiv Prepr. arXiv1705.10229},
title = {{Latent Intention Dialogue Models}},
year = {2017}
}
@article{wen2016network,
author = {Wen, Tsung-Hsien and Vandyke, David and Mrksic, Nikola and Gasic, Milica and Rojas-Barahona, Lina M and Su, Pei-Hao and Ultes, Stefan and Young, Steve},
journal = {arXiv Prepr. arXiv1604.04562},
title = {{A network-based end-to-end trainable task-oriented dialogue system}},
year = {2016}
}
@online{Mrkšić2017,
author = {Mrk{\v{s}}i{\'{c}}, Nikola and S{\'{e}}aghdha, Diarmuid {\'{O}} and Wen, Tsung-Hsien and Thomson, Blaise and Young, Steve},
title = {{Neural Belief Tracker: Data-Driven Dialogue State Tracking}},
year = {2017}
}
@inproceedings{Hakkani-Tur2016,
abstract = {Sequence-to-sequence deep learning has recently emerged as a new paradigm in supervised learning for spoken language understanding. However, most of the previous studies ex-plored this framework for building single domain models for each task, such as slot filling or domain classification, com-paring deep learning based approaches with conventional ones like conditional random fields. This paper proposes a holistic multi-domain, multi-task (i.e. slot filling, domain and intent detection) modeling approach to estimate complete semantic frames for all user utterances addressed to a conversational sys-tem, demonstrating the distinctive power of deep learning meth-ods, namely bi-directional recurrent neural network (RNN) with long-short term memory (LSTM) cells (RNN-LSTM) to handle such complexity. The contributions of the presented work are three-fold: (i) we propose an RNN-LSTM architecture for joint modeling of slot filling, intent determination, and domain clas-sification; (ii) we build a joint multi-domain model enabling multi-task deep learning where the data from each domain re-inforces each other; (iii) we investigate alternative architectures for modeling lexical context in spoken language understanding. In addition to the simplicity of the single model framework, ex-perimental results show the power of such an approach on Mi-crosoft Cortana real user data over alternative methods based on single domain/task deep learning.},
author = {Hakkani-T{\"{u}}r, Dilek and Tur, Gokhan and Celikyilmaz, Asli and Chen, Yun Nung and Gao, Jianfeng and Deng, Li and Wang, Ye Yi},
booktitle = {Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH},
doi = {10.21437/Interspeech.2016-402},
issn = {19909772},
keywords = {Joint modeling,Long short term memory,Multi-domain language understanding,Recurrent neural networks},
pages = {715--719},
title = {{Multi-domain joint semantic frame parsing using bi-directional RNN-LSTM}},
volume = {08-12-September-2016},
year = {2016}
}
@article{Fyshe2016,
abstract = {As a person reads, the brain performs complex operations to create higher order semantic representations from individual words. While these steps are effortless for competent readers, we are only beginning to understand how the brain performs these actions. Here, we explore semantic composition using magnetoencephalography (MEG) recordings of people reading adjective-noun phrases presented one word at a time. We track the neural representation of semantic information over time, through different brain regions. Our results reveal several novel findings: 1) the neural representation of adjective semantics observed during adjective reading is reactivated after phrase reading, with remarkable consistency, 2) a neural representation of the adjective is also present during noun presentation, but this neural representation is the reverse of that observed during adjective presentation 3) the neural representation of adjective semantics are oscillatory and entrained to alpha band frequencies. We also introduce a new method for analyzing brain image time series called Time Generalized Averaging. Taken together, these results paint a picture of information flow in the brain as phrases are read and understood.},
author = {Fyshe, Alona and Sudre, Gustavo and Wehbe, Leila and Rafidi, Nicole and Mitchell, Tom M},
journal = {bioRxiv},
keywords = {language comprehension,magnetoencephalography,oscillatory brain dynamics,semantic composition,semantic representations},
title = {{The Semantics of Adjective Noun Phrases in the Human Brain}},
year = {2016}
}
@inproceedings{LeilaWehbe2014,
abstract = {Many statistical models for natural language processing exist, including context-based neural networks that (1) model the previously seen context as a latent feature vector, (2) integrate successive words into the context using some learned representation (embedding), and (3) compute output probabilities for incoming words given the context. On the other hand, brain imaging studies have suggested that during reading, the brain (a) continuously builds a context from the successive words and every time it encounters a word it (b) fetches its properties from memory and (c) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is. This hints to a parallelism between the neural networks and the brain in modeling context (1 and a), representing the incoming words (2 and b) and integrating it (3 and c). We explore this parallelism to better understand the brain processes and the neural networks representations. We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography (MEG) when subjects read a story. For that purpose we apply the neural network to the same text the subjects are reading, and explore the ability of these three vector representations to predict the observed word-by-word brain activity.$\backslash$r$\backslash$n$\backslash$r$\backslash$nOur novel results show that: before a new word i is read, brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context. Secondly, the neural network embedding of word i can predict the MEG activity when word i is presented to the subject, revealing that it is correlated with the brain's own representation of word i. Moreover, we obtain that the activity is predicted in different regions of the brain with varying delay. The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions. Finally, we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i, as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context.},
author = {{Leila Wehbe} and {Ashish Vaswani} and {Kevin Knight} and {Tom Mitchell}},
booktitle = {Proc. 2014 Conf. Empir. Methods Nat. Lang. Process.},
doi = {10.1016/j.clinph.2006.07.316},
issn = {1388-2457},
pages = {233--243},
pmid = {17008126},
title = {{Aligning context-based statistical models of language with brain activity during reading}},
url = {http://emnlp2014.org/papers/pdf/EMNLP2014030.pdf{\%}5Cnhttp://aclweb.org/anthology/D/D14/D14-1030.pdf},
year = {2014}
}
@article{Just2010,
abstract = {This article describes the discovery of a set of biologically-driven semantic dimensions underlying the neural representation of concrete nouns, and then demonstrates how a resulting theory of noun representation can be used to identify simple thoughts through their fMRI patterns. We use factor analysis of fMRI brain imaging data to reveal the biological representation of individual concrete nouns like apple, in the absence of any pictorial stimuli. From this analysis emerge three main semantic factors underpinning the neural representation of nouns naming physical objects, which we label manipulation, shelter, and eating. Each factor is neurally represented in 3-4 different brain locations that correspond to a cortical network that co-activates in non-linguistic tasks, such as tool use pantomime for the manipulation factor. Several converging methods, such as the use of behavioral ratings of word meaning and text corpus characteristics, provide independent evidence of the centrality of these factors to the representations. The factors are then used with machine learning classifier techniques to show that the fMRI-measured brain representation of an individual concrete noun like apple can be identified with good accuracy from among 60 candidate words, using only the fMRI activity in the 16 locations associated with these factors. To further demonstrate the generativity of the proposed account, a theory-based model is developed to predict the brain activation patterns for words to which the algorithm has not been previously exposed. The methods, findings, and theory constitute a new approach of using brain activity for understanding how object concepts are represented in the mind.},
author = {Just, Marcel Adam and Cherkassky, Vladimir L. and Aryal, Sandesh and Mitchell, Tom M.},
doi = {10.1371/journal.pone.0008622},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS One},
number = {1},
pmid = {20084104},
title = {{A neurosemantic theory of concrete noun representation based on the underlying brain codes}},
volume = {5},
year = {2010}
}
@article{Mitchell2008,
abstract = {The question of how the human brain represents conceptual knowledge has been debated in many scientific fields. Brain imaging studies have shown that different spatial patterns of neural activation are associated with thinking about different semantic categories of pictures and words (for example, tools, buildings, and animals). We present a computational model that predicts the functional magnetic resonance imaging (fMRI) neural activation associated with words for which fMRI data are not yet available. This model is trained with a combination of data from a trillion-word text corpus and observed fMRI data associated with viewing several dozen concrete nouns. Once trained, the model predicts fMRI activation for thousands of other concrete nouns in the text corpus, with highly significant accuracies over the 60 nouns for which we currently have fMRI data.},
author = {Mitchell, Tom M. and Shinkareva, Svetlana V. and Carlson, Andrew and Chang, Kai-Min and Malave, Vicente L. and Mason, Robert A. and Just, Marcel Adam},
doi = {10.1126/science.1152876},
isbn = {1095-9203},
issn = {0036-8075, 1095-9203},
journal = {Science (80-. ).},
keywords = {Mendeley Import (May 18)/ZZ-OTHERS},
number = {5880},
pages = {1191--1195},
pmid = {18511683},
title = {{Predicting Human Brain Activity Associated with the Meanings of Nouns}},
url = {http://www.sciencemag.org/content/320/5880/1191{\%}0Ahttp://www.sciencemag.org/content/320/5880/1191.short},
volume = {320},
year = {2008}
}
@article{Just2014,
abstract = {Autism is a psychiatric/neurological condition in which alterations in social interaction (among other symptoms) are diagnosed by behavioral psychiatric methods. The main goal of this study was to determine how the neural representations and meanings of social concepts (such as to insult) are altered in autism. A second goal was to determine whether these alterations can serve as neurocognitive markers of autism. The approach is based on previous advances in fMRI analysis methods that permit (a) the identification of a concept, such as the thought of a physical object, from its fMRI pattern, and (b) the ability to assess the semantic content of a concept from its fMRI pattern. These factor analysis and machine learning methods were applied to the fMRI activation patterns of 17 adults with high-functioning autism and matched controls, scanned while thinking about 16 social interactions. One prominent neural representation factor that emerged (manifested mainly in posterior midline regions) was related to self-representation, but this factor was present only for the control participants, and was near-absent in the autism group. Moreover, machine learning algorithms classified individuals as autistic or control with 97{\%} accuracy from their fMRI neurocognitive markers. The findings suggest that psychiatric alterations of thought can begin to be biologically understood by assessing the form and content of the altered thought's underlying brain activation patterns.},
author = {Just, Marcel Adam and Cherkassky, Vladimir L. and Buchweitz, Augusto and Keller, Timothy A. and Mitchell, Tom M.},
doi = {10.1371/journal.pone.0113879},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS One},
number = {12},
pmid = {25461818},
title = {{Identifying autism from neural representations of social interactions: Neurocognitive markers of autism}},
volume = {9},
year = {2014}
}
@article{Mrk2015,
abstract = {Dialog state tracking is a key component of many modern dialog systems, most of which are designed with a single, well-defined domain in mind. This paper shows that dialog data drawn from different dialog domains can be used to train a general belief tracking model which can operate across all of these domains, exhibiting superior performance to each of the domain-specific models. We propose a training procedure which uses out-of-domain data to initialise belief tracking models for entirely new domains. This procedure leads to improvements in belief tracking performance regardless of the amount of in-domain data available for training the model.},
archivePrefix = {arXiv},
arxivId = {1506.07190},
author = {Mrkˇ, Nikola and Diarmuid, O},
eprint = {1506.07190},
journal = {Proc. 53rd Annu. Meet. Assoc. Comput. Linguist. 7th Int. Jt. Conf. Nat. Lang. Process. (Volume 1 Long Pap.},
pages = {794--799},
title = {{Multi-domain Dialog State Tracking using Recurrent Neural Networks}},
year = {2015}
}
@inproceedings{Bui2008,
abstract = {We propose a novel approach to developing a tractable affective dialogue model for probabilistic frame-based dialogue systems. The affective dialogue model, based on Partially Observable Markov Decision Process (POMDP) and Dynamic Decision Network (DDN) techniques, is composed of two main parts: the slot-level dialogue manager and the global dialogue manager. It has two new features: (1) being able to deal with a large number of slots and (2) being able to take into account some aspects of the user's affective state in deriving the adaptive dialogue strategies. Our implemented prototype dialogue manager can handle hundreds of slots, where each individual slot might have hundreds of values. Our approach is illustrated through a route navigation example in the crisis management domain. We conducted various experiments to evaluate our approach and to compare it with approximate POMDP techniques and handcrafted policies. The experimental results showed that the DDN-POMDP policy outperforms three handcrafted policies when the user's action error is induced by stress as well as when the observation error increases. Further, performance of the one-step look-ahead DDN-POMDP policy after optimizing its internal reward is close to state-of-the-art approximate POMDP counterparts. {\textcopyright} 2009 Cambridge University Press.},
author = {Bui, Trung H. and Poel, Mannes and NijholtM, Anton and Zwiers, Job},
booktitle = {Belgian/Netherlands Artif. Intell. Conf.},
doi = {10.1017/S1351324908005032},
issn = {15687805},
pages = {289--290},
title = {{A Tractable Hybrid DDN-POMDP approach to Affective Dialogue Modeling for Probabilistic Frame-based Dialogue Systems}},
year = {2008}
}
@article{Williams2007,
abstract = {In a spoken dialog system, determining which action a machine should take in a given situation is a difficult problem because automatic speech recognition is unreliable and hence the state of the conversation can never be known with certainty. Much of the research in spoken dialog systems centres on mitigating this uncertainty and recent work has focussed on three largely disparate techniques: parallel dialog state hypotheses, local use of confidence scores, and automated planning. While in isolation each of these approaches can improve action selection, taken together they currently lack a unified statistical framework that admits global optimization. In this paper we cast a spoken dialog system as a partially observable Markov decision process (POMDP). We show how this formulation unifies and extends existing techniques to form a single principled framework. A number of illustrations are used to show qualitatively the potential benefits of POMDPs compared to existing techniques, and empirical results from dialog simulations are presented which demonstrate significant quantitative gains. Finally, some of the key challenges to advancing this method - in particular scalability - are briefly outlined. ?? 2006 Elsevier Ltd. All rights reserved.},
author = {Williams, Jason D. and Young, Steve},
doi = {10.1016/j.csl.2006.06.008},
isbn = {08852308 (ISSN)},
issn = {08852308},
journal = {Comput. Speech Lang.},
keywords = {Decision theory,Dialog management,Markov decision processes,Planning under uncertainty,Spoken dialog system,User modelling},
number = {2},
pages = {393--422},
title = {{Partially observable Markov decision processes for spoken dialog systems}},
volume = {21},
year = {2007}
}
@article{Young2013,
abstract = {—Statistical dialogue systems are motivated by the need for a data-driven framework that reduces the cost of laboriously hand-crafting complex dialogue managers and that provides robustness against the errors created by speech recog-nisers operating in noisy environments. By including an explicit Bayesian model of uncertainty and by optimising the policy via a reward-driven process, partially observable Markov decision processes (POMDPs) provide such a framework. However, ex-act model representation and optimisation is computationally intractable. Hence, the practical application of POMDP-based systems requires efficient algorithms and carefully constructed approximations. This review article provides an overview of the current state of the art in the development of POMDP-based spoken dialogue systems.},
author = {Young, Steve and Ga{\v{s}}i, Milica and Thomson, Blaise and Williams, Jason D},
doi = {10.1016/j.csl.2009.04.001},
isbn = {0885-2308},
issn = {08852308},
journal = {Proc Ieee},
keywords = {Index Terms—Spoken dialogue systems,POMDP,belief monitoring,policy optimisation,reinforce-ment learning},
number = {5},
pages = {1160--1179},
title = {{POMDP-based Statistical Spoken Dialogue Systems: a Review}},
volume = {101},
year = {2013}
}
@article{Henderson,
abstract = {While belief tracking is known to be im-portant in allowing statistical dialog sys-tems to manage dialogs in a highly robust manner, until recently little attention has been given to analysing the behaviour of belief tracking techniques. The Dialogue State Tracking Challenge has allowed for such an analysis, comparing multiple be-lief tracking approaches on a shared task. Recent success in using deep learning for speech research motivates the Deep Neu-ral Network approach presented here. The model parameters can be learnt by directly maximising the likelihood of the training data. The paper explores some aspects of the training, and the resulting tracker is found to perform competitively, particu-larly on a corpus of dialogs from a system not found in the training.},
author = {Henderson, Matthew and Thomson, Blaise and Young, Steve},
journal = {Proc. SIGdial},
pages = {467--471},
title = {{Deep Neural Network Approach for the Dialog State Tracking Challenge}},
year = {2013}
}
@article{Roy2000,
abstract = {Spoken dialogue managers have benefited from using stochastic planners such as Markov Decision Processes (MDPs). However, so far, MDPs do not handle well noisy and ambiguous speech utterances. We use a Partially Observable Markov Decision Process (POMDP)-style approach to generate dialogue strategies by inverting the notion of dialogue state; the state represents the user's intentions, rather than the system state. We demonstrate that under the same noisy conditions, a POMDP dialogue manager makes fewer mistakes than an MDP dialogue manager. Furthermore, as the quality of speech recognition degrades, the POMDP dialogue manager automatically adjusts the policy.},
author = {Roy, Nicholas and Pineau, Joelle and Thrun, Sebastian},
doi = {10.3115/1075218.1075231},
journal = {Proc. 38th Annu. Meet. Assoc. Comput. Linguist.},
pages = {93--100},
title = {{Spoken Dialogue Management Using Probabilistic Reasoning}},
url = {http://dx.doi.org/10.3115/1075218.1075231},
year = {2000}
}
@article{Fridriksson2016,
abstract = {Several dual route models of human speech processing have been proposed suggesting a large-scale anatomical division between cortical regions that support motor-phonological aspects vs. lexical-semantic aspects of speech processing. However, to date, there is no complete agreement on what areas subserve each route or the nature of interactions across these routes that enables human speech processing. Relying on an extensive behavioral and neuroimaging assessment of a large sample of stroke survivors, we used a data-driven approach using principal components analysis of lesion-symptom mapping to identify brain regions crucial for performance on clusters of behavioral tasks without a priori separation into task types. Distinct anatomical boundaries were revealed between a dorsal frontoparietal stream and a ventral temporal-frontal stream associated with separate components. Collapsing over the tasks primarily supported by these streams, we characterize the dorsal stream as a form-to-articulation pathway and the ventral stream as a form-to-meaning pathway. This characterization of the division in the data reflects both the overlap between tasks supported by the two streams as well as the observation that there is a bias for phonological production tasks supported by the dorsal stream and lexical-semantic comprehension tasks supported by the ventral stream. As such, our findings show a division between two processing routes that underlie human speech processing and provide an empirical foundation for studying potential computational differences that distinguish between the two routes.},
author = {Fridriksson, Julius and Yourganov, Grigori and Bonilha, Leonardo and Basilakos, Alexandra and {Den Ouden}, Dirk-Bart and Rorden, Christopher},
doi = {10.1073/pnas.1614038114},
issn = {1091-6490},
journal = {Proc. Natl. Acad. Sci. U. S. A.},
keywords = {aphasia,speech comprehension,speech processing,speech production,voxel-based lesion-symptom mapping},
month = {dec},
number = {52},
pages = {15108--15113},
pmid = {27956600},
publisher = {National Academy of Sciences},
title = {{Revealing the dual streams of speech processing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27956600 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5206517},
volume = {113},
year = {2016}
}
@article{Hickok2007,
abstract = {Despite decades of research, the functional neuroanatomy of speech processing has been difficult to characterize. A major impediment to progress may have been the failure to consider task effects when mapping speech-related processing systems. We outline a dual-stream model of speech processing that remedies this situation. In this model, a ventral stream processes speech signals for comprehension, and a dorsal stream maps acoustic speech signals to frontal lobe articulatory networks. The model assumes that the ventral stream is largely bilaterally organized--although there are important computational differences between the left- and right-hemisphere systems--and that the dorsal stream is strongly left-hemisphere dominant.},
author = {Hickok, Gregory and Poeppel, David},
doi = {10.1038/nrn2113},
isbn = {1471-003X (Print)},
issn = {1471-003X},
journal = {Nat. Rev. Neurosci.},
keywords = {Brain Mapping,Cerebral Cortex,Cerebral Cortex: physiology,Humans,Models,Psychological,Speech,Speech: physiology,Verbal Behavior,Verbal Behavior: physiology},
number = {5},
pages = {393--402},
pmid = {17431404},
title = {{The cortical organization of speech processing.}},
url = {http://dx.doi.org/10.1038/nrn2113},
volume = {8},
year = {2007}
}
@article{Damasio1980,
author = {Damasio, H and Damasio, A R},
issn = {0006-8950},
journal = {Brain},
month = {jun},
number = {2},
pages = {337--50},
pmid = {7397481},
title = {{The anatomical basis of conduction aphasia.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7397481},
volume = {103},
year = {1980}
}
@article{Huth2016,
abstract = {The meaning of language is represented in regions of the cerebral cortex collectively known as the ‘semantic system'. However, little of the semantic system has been mapped comprehensively, and the semantic selectivity of most regions is unknown. Here we systematically map semantic selectivity across the cortex using voxel-wise modelling of functional MRI (fMRI) data collected while subjects listened to hours of narrative stories. We show that the semantic system is organized into intricate patterns that seem to be consistent across individuals. We then use a novel generative model to create a detailed semantic atlas. Our results suggest that most areas within the semantic system represent information about specific semantic domains, or groups of related concepts, and our atlas shows which domains are represented in each area. This study demonstrates that data-driven methods—commonplace in studies of human neuroanatomy and functional connectivity—provide a powerful and efficient means for mapping functional representations in the brain.},
author = {Huth, Alexander G and de Heer, Wendy A and Griffiths, Thomas L and Theunissen, Fr{\'{e}}d{\'{e}}ric E and Gallant, Jack L},
issn = {0028-0836},
journal = {Nature},
month = {apr},
number = {7600},
pages = {453--458},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
title = {{Natural speech reveals the semantic maps that tile human cerebral cortex}},
volume = {532},
year = {2016}
}
